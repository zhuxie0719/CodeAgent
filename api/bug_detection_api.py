"""
ç®€åŒ–çš„BugDetectionAgent APIæœåŠ¡
åªä¿ç•™æ¥å£è°ƒç”¨ï¼Œå…·ä½“é€»è¾‘åœ¨agentså±‚
"""

import asyncio
import uuid
import os
import json
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent.parent))

# å¯¼å…¥çœŸæ­£çš„BugDetectionAgent
from agents.bug_detection_agent.agent import BugDetectionAgent
# é¢„ç•™å…¶å®ƒAgentå¯¼å…¥ï¼ˆå½•å±å±•ç¤ºç”¨é€”ï¼Œæš‚ä¸å¯ç”¨ï¼‰
# from agents.fix_execution_agent.agent import FixExecutionAgent
# from agents.test_validation_agent.agent import TestValidationAgent
# from agents.code_analysis_agent.agent import CodeAnalysisAgent
# from agents.code_quality_agent.agent import CodeQualityAgent
# from agents.performance_optimization_agent.agent import PerformanceOptimizationAgent
from coordinator.coordinator import Coordinator

# ç®€åŒ–çš„è®¾ç½®
class Settings:
    AGENTS = {"bug_detection_agent": {"enabled": True}}

settings = Settings()

# æ•°æ®æ¨¡å‹
class BaseResponse(BaseModel):
    """åŸºç¡€å“åº”æ¨¡å‹"""
    success: bool = Field(True, description="æ˜¯å¦æˆåŠŸ")
    message: str = Field("æ“ä½œæˆåŠŸ", description="å“åº”æ¶ˆæ¯")
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat(), description="æ—¶é—´æˆ³")
    data: Optional[Dict[str, Any]] = Field(None, description="å“åº”æ•°æ®")

class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”æ¨¡å‹"""
    status: str = Field(..., description="æœåŠ¡çŠ¶æ€")
    message: str = Field(..., description="çŠ¶æ€æ¶ˆæ¯")
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat(), description="æ—¶é—´æˆ³")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="AI Agent ç¼ºé™·æ£€æµ‹ API",
    description="ä¸“æ³¨äºç¼ºé™·æ£€æµ‹çš„APIæœåŠ¡",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å®ä¾‹
bug_detection_agent = None
# fix_execution_agent = None
# test_validation_agent = None
# code_analysis_agent = None
# code_quality_agent = None
# performance_optimization_agent = None
coordinator = None

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    global bug_detection_agent, coordinator
    try:
        config = settings.AGENTS.get("bug_detection_agent", {})
        bug_detection_agent = BugDetectionAgent(config)
        await bug_detection_agent.start()
        print("BugDetectionAgent å¯åŠ¨æˆåŠŸ")
    except Exception as e:
        print(f"BugDetectionAgent å¯åŠ¨å¤±è´¥: {e}")
        bug_detection_agent = None

    try:
        coordinator = Coordinator(config={})
        await coordinator.start()
        print("Coordinator å¯åŠ¨æˆåŠŸ")
    except Exception as e:
        print(f"Coordinator å¯åŠ¨å¤±è´¥: {e}")
        coordinator = None

    # æ³¨å†Œæ£€æµ‹Agentåˆ°åè°ƒä¸­å¿ƒï¼ˆå…³é”®ï¼šå¦åˆ™æ— æ³•åˆ†é…ä»»åŠ¡ç»™çœŸå®Agentï¼‰
    try:
        if coordinator and bug_detection_agent:
            await coordinator.register_agent('bug_detection_agent', bug_detection_agent)
            print("BugDetectionAgent å·²æ³¨å†Œåˆ° Coordinator")
    except Exception as e:
        print(f"æ³¨å†Œ BugDetectionAgent å¤±è´¥: {e}")

    # é¢„ç•™ï¼šå…¶å®ƒAgentå¯åŠ¨ä¸æ³¨å†Œï¼ˆå½•å±æ³¨é‡Šå ä½ï¼Œéœ€è¦æ—¶å–æ¶ˆæ³¨é‡Šå³å¯ï¼‰
    # global fix_execution_agent, test_validation_agent, code_analysis_agent, code_quality_agent, performance_optimization_agent
    # try:
    #     fix_execution_agent = FixExecutionAgent(config={})
    #     await fix_execution_agent.start()
    #     if coordinator:
    #         await coordinator.register_agent('fix_execution_agent', fix_execution_agent)
    #     print("FixExecutionAgent å¯åŠ¨å¹¶æ³¨å†ŒæˆåŠŸ")
    # except Exception as e:
    #     print(f"FixExecutionAgent å¯åŠ¨/æ³¨å†Œå¤±è´¥: {e}")
    # try:
    #     test_validation_agent = TestValidationAgent(config={})
    #     await test_validation_agent.start()
    #     if coordinator:
    #         await coordinator.register_agent('test_validation_agent', test_validation_agent)
    #     print("TestValidationAgent å¯åŠ¨å¹¶æ³¨å†ŒæˆåŠŸ")
    # except Exception as e:
    #     print(f"TestValidationAgent å¯åŠ¨/æ³¨å†Œå¤±è´¥: {e}")
    # try:
    #     code_analysis_agent = CodeAnalysisAgent(config={})
    #     await code_analysis_agent.start()
    #     if coordinator:
    #         await coordinator.register_agent('code_analysis_agent', code_analysis_agent)
    #     print("CodeAnalysisAgent å¯åŠ¨å¹¶æ³¨å†ŒæˆåŠŸ")
    # except Exception as e:
    #     print(f"CodeAnalysisAgent å¯åŠ¨/æ³¨å†Œå¤±è´¥: {e}")
    # try:
    #     code_quality_agent = CodeQualityAgent(config={})
    #     await code_quality_agent.start()
    #     if coordinator:
    #         await coordinator.register_agent('code_quality_agent', code_quality_agent)
    #     print("CodeQualityAgent å¯åŠ¨å¹¶æ³¨å†ŒæˆåŠŸ")
    # except Exception as e:
    #     print(f"CodeQualityAgent å¯åŠ¨/æ³¨å†Œå¤±è´¥: {e}")
    # try:
    #     performance_optimization_agent = PerformanceOptimizationAgent(config={})
    #     await performance_optimization_agent.start()
    #     if coordinator:
    #         await coordinator.register_agent('performance_optimization_agent', performance_optimization_agent)
    #     print("PerformanceOptimizationAgent å¯åŠ¨å¹¶æ³¨å†ŒæˆåŠŸ")
    # except Exception as e:
    #     print(f"PerformanceOptimizationAgent å¯åŠ¨/æ³¨å†Œå¤±è´¥: {e}")

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    global bug_detection_agent, coordinator
    if bug_detection_agent:
        await bug_detection_agent.stop()
        print("BugDetectionAgent å·²åœæ­¢")
    # é¢„ç•™ï¼šå…¶å®ƒAgentåœæ­¢ï¼ˆéœ€è¦æ—¶å–æ¶ˆæ³¨é‡Šï¼‰
    # if fix_execution_agent:
    #     await fix_execution_agent.stop()
    #     print("FixExecutionAgent å·²åœæ­¢")
    # if test_validation_agent:
    #     await test_validation_agent.stop()
    #     print("TestValidationAgent å·²åœæ­¢")
    # if code_analysis_agent:
    #     await code_analysis_agent.stop()
    #     print("CodeAnalysisAgent å·²åœæ­¢")
    # if code_quality_agent:
    #     await code_quality_agent.stop()
    #     print("CodeQualityAgent å·²åœæ­¢")
    # if performance_optimization_agent:
    #     await performance_optimization_agent.stop()
    #     print("PerformanceOptimizationAgent å·²åœæ­¢")
    if coordinator:
        await coordinator.stop()
        print("Coordinator å·²åœæ­¢")

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    global bug_detection_agent, coordinator
    
    if bug_detection_agent and coordinator:
        agent_status = bug_detection_agent.get_status()
        coord_status = await coordinator.health_check()
        return HealthResponse(
            status="healthy",
            message=f"APIæœåŠ¡è¿è¡Œæ­£å¸¸ï¼ŒAgentçŠ¶æ€: {agent_status['status']}ï¼ŒCoordinator: running={coord_status['is_running']}",
            timestamp=datetime.now().isoformat()
        )
    else:
        return HealthResponse(
            status="error",
            message="BugDetectionAgent æˆ– Coordinator æœªå¯åŠ¨",
            timestamp=datetime.now().isoformat()
        )

@app.post("/api/v1/detection/upload", response_model=BaseResponse)
async def upload_file_for_detection(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    enable_static: bool = Query(True, description="å¯ç”¨è‡ªå®šä¹‰é™æ€æ£€æµ‹"),
    enable_pylint: bool = Query(True, description="å¯ç”¨Pylintæ£€æµ‹"),
    enable_flake8: bool = Query(True, description="å¯ç”¨Flake8æ£€æµ‹"),
    enable_bandit: bool = Query(True, description="å¯ç”¨Banditå®‰å…¨æ£€æµ‹"),
    enable_mypy: bool = Query(True, description="å¯ç”¨Mypyç±»å‹æ£€æŸ¥"),
    enable_ai_analysis: bool = Query(True, description="å¯ç”¨AIåˆ†æ"),
    analysis_type: str = Query("file", description="åˆ†æç±»å‹: file(å•æ–‡ä»¶) æˆ– project(é¡¹ç›®)")
):
    """ä¸Šä¼ æ–‡ä»¶è¿›è¡Œç¼ºé™·æ£€æµ‹ - æ”¯æŒå¤æ‚é¡¹ç›®å‹ç¼©åŒ…"""
    global coordinator
    
    if not coordinator:
        raise HTTPException(status_code=500, detail="Coordinator æœªå¯åŠ¨")
    
    # éªŒè¯æ–‡ä»¶å¤§å°
    content = await file.read()
    file_size = len(content)
    
    # æ ¹æ®åˆ†æç±»å‹è®¾ç½®ä¸åŒçš„é™åˆ¶
    if analysis_type == "project":
        max_size = 100 * 1024 * 1024  # 100MB for projects
        supported_extensions = ['.zip', '.tar', '.tar.gz', '.rar', '.7z']
    else:
        max_size = 10 * 1024 * 1024  # 10MB for single files
        supported_extensions = ['.py', '.java', '.c', '.cpp', '.h', '.hpp', '.js', '.ts', '.go']
    
    if file_size > max_size:
        raise HTTPException(status_code=413, detail=f"æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§æ”¯æŒ{max_size // (1024*1024)}MB")
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    file_extension = Path(file.filename).suffix.lower()
    if file_extension not in supported_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚æ”¯æŒçš„ç±»å‹: {', '.join(supported_extensions)}"
        )
    
    # ä¿å­˜æ–‡ä»¶
    upload_dir = Path("uploads")
    upload_dir.mkdir(exist_ok=True)
    file_path = upload_dir / f"{file.filename}"
    
    with open(file_path, "wb") as f:
        f.write(content)
    
    # æ ¹æ®åˆ†æç±»å‹åˆ›å»ºæ£€æµ‹ä»»åŠ¡
    if analysis_type == "file":
        # å•æ–‡ä»¶æ£€æµ‹
        task_data = {
            "file_path": str(file_path),
            "analysis_type": "file",
            "options": {
                "enable_static": enable_static,
                "enable_pylint": enable_pylint,
                "enable_flake8": enable_flake8,
                "enable_bandit": enable_bandit,
                "enable_mypy": enable_mypy,
                "enable_ai_analysis": enable_ai_analysis
            }
        }
    elif analysis_type == "project":
        # é¡¹ç›®æ£€æµ‹
        task_data = {
            # æç¤ºï¼šè¿™é‡Œä¼ å…¥ä¸Šä¼ çš„å‹ç¼©åŒ…è·¯å¾„ï¼Œç”±Agentè´Ÿè´£è§£å‹
            "file_path": str(file_path),
            "analysis_type": "project",
            "options": {
                "enable_static": enable_static,
                "enable_pylint": enable_pylint,
                "enable_flake8": enable_flake8,
                "enable_bandit": enable_bandit,
                "enable_mypy": enable_mypy,
                "enable_ai_analysis": enable_ai_analysis
            }
        }
    else:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„åˆ†æç±»å‹: {analysis_type}")
    
    try:
        # é€šè¿‡åè°ƒä¸­å¿ƒåˆ›å»º detect_bugs ä»»åŠ¡å¹¶åˆ†é…ç»™ bug_detection_agent
        task_id = await coordinator.create_task('detect_bugs', task_data)
        await coordinator.assign_task(task_id, 'bug_detection_agent')

        # åå°ä»»åŠ¡ï¼šåŸºäºåè°ƒä¸­å¿ƒçš„ä»»åŠ¡ç»“æœç”ŸæˆæŠ¥å‘Šä¸ç»“æ„åŒ–æ•°æ®
        background_tasks.add_task(generate_report_task, task_id, str(file_path))
        background_tasks.add_task(store_structured_data, task_id, str(file_path), analysis_type)

        return BaseResponse(
            message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå¼€å§‹æ£€æµ‹",
            data={
                "task_id": task_id,
                "filename": file.filename,
                "file_size": file_size,
                "agent_id": "bug_detection_agent",
                "analysis_type": analysis_type
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æäº¤æ£€æµ‹ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.get("/api/v1/tasks/{task_id}", response_model=BaseResponse)
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    global coordinator
    
    if not coordinator:
        raise HTTPException(status_code=500, detail="Coordinator æœªå¯åŠ¨")
    
    try:
        task = coordinator.task_manager.tasks.get(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        data = {
            "task_id": task_id,
            "status": task['status'].value,
            "created_at": task['created_at'].isoformat(),
            "started_at": task['started_at'].isoformat() if task['started_at'] else None,
            "completed_at": task['completed_at'].isoformat() if task['completed_at'] else None,
            "result": task['result'],
            "error": task['error']
        }
        return BaseResponse(message="è·å–ä»»åŠ¡çŠ¶æ€æˆåŠŸ", data=data)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")

@app.get("/api/v1/detection/rules", response_model=BaseResponse)
async def get_detection_rules():
    """è·å–æ£€æµ‹è§„åˆ™"""
    global bug_detection_agent
    if not bug_detection_agent:
        raise HTTPException(status_code=500, detail="BugDetectionAgent æœªå¯åŠ¨")
    try:
        rules = await bug_detection_agent.get_detection_rules()
        return BaseResponse(message="è·å–æ£€æµ‹è§„åˆ™æˆåŠŸ", data=rules)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ£€æµ‹è§„åˆ™å¤±è´¥: {str(e)}")

@app.get("/api/v1/ai-reports/{task_id}")
async def get_ai_report(task_id: str):
    """è·å–AIç”Ÿæˆçš„è‡ªç„¶è¯­è¨€æŠ¥å‘Š"""
    global coordinator
    if not coordinator:
        raise HTTPException(status_code=500, detail="Coordinator æœªå¯åŠ¨")
    try:
        # è·å–ä»»åŠ¡çŠ¶æ€
        task = coordinator.task_manager.tasks.get(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        if task['status'].value != "completed":
            raise HTTPException(status_code=400, detail="ä»»åŠ¡å°šæœªå®Œæˆ")
        
        # æ£€æŸ¥AIæŠ¥å‘Šæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        ai_report_path = Path("reports") / f"ai_report_{task_id}.md"
        
        if ai_report_path.exists():
            # è¯»å–AIæŠ¥å‘Šå†…å®¹
            with open(ai_report_path, 'r', encoding='utf-8') as f:
                ai_report_content = f.read()
            
            return BaseResponse(
                message="è·å–AIæŠ¥å‘ŠæˆåŠŸ",
                data={
                    "task_id": task_id,
                    "ai_report": ai_report_content,
                    "report_type": "markdown"
                }
            )
        else:
            # å¦‚æœæ²¡æœ‰AIæŠ¥å‘Šæ–‡ä»¶ï¼Œå®æ—¶ç”Ÿæˆä¸€ä¸ª
            detection_results = (task.get('result') or {}).get("detection_results", {})
            file_path = (task.get('result') or {}).get("file_path", "")
            
            if detection_results:
                ai_report = await generate_ai_report(detection_results, file_path)
                
                # ä¿å­˜AIæŠ¥å‘Š
                ai_report_path.parent.mkdir(exist_ok=True)
                with open(ai_report_path, 'w', encoding='utf-8') as f:
                    f.write(ai_report)
                
                return BaseResponse(
                    message="è·å–AIæŠ¥å‘ŠæˆåŠŸ",
                    data={
                        "task_id": task_id,
                        "ai_report": ai_report,
                        "report_type": "markdown"
                    }
                )
            else:
                raise HTTPException(status_code=404, detail="æ£€æµ‹ç»“æœä¸å­˜åœ¨")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–AIæŠ¥å‘Šå¤±è´¥: {str(e)}")

@app.get("/api/v1/ai-reports/{task_id}/download")
async def download_ai_report(task_id: str):
    """ä¸‹è½½AIæŠ¥å‘Šæ–‡ä»¶"""
    try:
        # æ£€æŸ¥AIæŠ¥å‘Šæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        ai_report_path = Path("reports") / f"ai_report_{task_id}.md"
        
        if not ai_report_path.exists():
            raise HTTPException(status_code=404, detail="AIæŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨")
        
        # è¿”å›æ–‡ä»¶ä¸‹è½½
        from fastapi.responses import FileResponse
        return FileResponse(
            path=str(ai_report_path),
            filename=f"ai_report_{task_id}.md",
            media_type="text/markdown"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½AIæŠ¥å‘Šå¤±è´¥: {str(e)}")

@app.get("/api/v1/structured-data/{task_id}", response_model=BaseResponse)
async def get_structured_data(task_id: str):
    """è·å–ç»“æ„åŒ–æ•°æ®ç»™ä¿®å¤agent"""
    try:
        # æ£€æŸ¥ç»“æ„åŒ–æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        structured_file = Path("structured_data") / f"structured_data_{task_id}.json"
        
        if not structured_file.exists():
            raise HTTPException(status_code=404, detail="ç»“æ„åŒ–æ•°æ®ä¸å­˜åœ¨")
        
        # è¯»å–ç»“æ„åŒ–æ•°æ®
        with open(structured_file, 'r', encoding='utf-8') as f:
            structured_data = json.load(f)
        
        return BaseResponse(
            message="è·å–ç»“æ„åŒ–æ•°æ®æˆåŠŸ",
            data=structured_data
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç»“æ„åŒ–æ•°æ®å¤±è´¥: {str(e)}")

@app.get("/api/v1/reports/{task_id}")
async def download_report(task_id: str):
    """ä¸‹è½½æ£€æµ‹æŠ¥å‘Š"""
    global coordinator
    if not coordinator:
        raise HTTPException(status_code=500, detail="Coordinator æœªå¯åŠ¨")
    
    try:
        # è·å–ä»»åŠ¡çŠ¶æ€
        task = coordinator.task_manager.tasks.get(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        if task['status'].value != "completed":
            raise HTTPException(status_code=400, detail="ä»»åŠ¡å°šæœªå®Œæˆ")
        
        # ç”ŸæˆæŠ¥å‘Š
        detection_results = (task.get('result') or {}).get("detection_results", {})
        file_path = (task.get('result') or {}).get("file_path", "")
        
        if not detection_results:
            raise HTTPException(status_code=404, detail="æ£€æµ‹ç»“æœä¸å­˜åœ¨")
        
        # æ£€æŸ¥BugDetectionAgentæ˜¯å¦æœ‰generate_downloadable_reportæ–¹æ³•
        if hasattr(bug_detection_agent, 'generate_downloadable_report'):
            report_path = await bug_detection_agent.generate_downloadable_report(detection_results, file_path)
        else:
            # å¦‚æœæ²¡æœ‰è¯¥æ–¹æ³•ï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æŠ¥å‘Š
            report_path = await create_simple_report(detection_results, file_path, task_id)
        
        if not report_path or not Path(report_path).exists():
            raise HTTPException(status_code=404, detail="æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨")
        
        # è¿”å›æ–‡ä»¶
        from fastapi.responses import FileResponse
        return FileResponse(
            path=report_path,
            filename=f"bug_detection_report_{task_id}.json",
            media_type="application/json"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æŠ¥å‘Šå¤±è´¥: {str(e)}")

async def create_simple_report(detection_results: Dict[str, Any], file_path: str, task_id: str) -> str:
    """åˆ›å»ºç®€åŒ–çš„æ£€æµ‹æŠ¥å‘Š"""
    try:
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = Path("reports")
        report_dir.mkdir(exist_ok=True)
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"bug_detection_report_{timestamp}.json"
        report_path = report_dir / filename
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_data = {
            "report_info": {
                "generated_at": datetime.now().isoformat(),
                "file_path": file_path,
                "task_id": task_id,
                "total_issues": detection_results.get("total_issues", 0),
                "summary": detection_results.get("summary", {}),
                "detection_tools": detection_results.get("detection_tools", [])
            },
            "issues": detection_results.get("issues", []),
            "statistics": {
                "by_severity": _get_issues_by_severity(detection_results.get("issues", [])),
                "by_type": _get_issues_by_type(detection_results.get("issues", [])),
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"ç®€åŒ–æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return str(report_path)
        
    except Exception as e:
        print(f"ç”Ÿæˆç®€åŒ–æŠ¥å‘Šå¤±è´¥: {e}")
        return None

def _get_issues_by_severity(issues: List[Dict[str, Any]]) -> Dict[str, int]:
    """æŒ‰ä¸¥é‡æ€§ç»Ÿè®¡é—®é¢˜"""
    severity_count = {}
    for issue in issues:
        severity = issue.get("severity", "info")
        severity_count[severity] = severity_count.get(severity, 0) + 1
    return severity_count

def _get_issues_by_type(issues: List[Dict[str, Any]]) -> Dict[str, int]:
    """æŒ‰ç±»å‹ç»Ÿè®¡é—®é¢˜"""
    type_count = {}
    for issue in issues:
        issue_type = issue.get("type", "unknown")
        type_count[issue_type] = type_count.get(issue_type, 0) + 1
    return type_count

async def generate_report_task(task_id: str, file_path: str):
    """åå°ä»»åŠ¡ï¼šç”Ÿæˆæ£€æµ‹æŠ¥å‘Š"""
    global coordinator
    
    try:
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        max_wait_time = 300  # 5åˆ†é’Ÿ
        wait_interval = 2    # 2ç§’
        waited_time = 0
        
        while waited_time < max_wait_time:
            task = coordinator.task_manager.tasks.get(task_id)
            if task and task['status'].value == "completed":
                break
            await asyncio.sleep(wait_interval)
            waited_time += wait_interval
        
        if waited_time >= max_wait_time:
            print(f"ä»»åŠ¡ {task_id} è¶…æ—¶ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return
        
        # ç”ŸæˆæŠ¥å‘Š
        detection_results = (task.get('result') or {}).get("detection_results", {})
        if detection_results:
            # ç”ŸæˆJSONæŠ¥å‘Š
            report_path = await create_simple_report(detection_results, file_path, task_id)
            
            if report_path:
                print(f"JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
    except Exception as e:
        print(f"ç”ŸæˆæŠ¥å‘Šä»»åŠ¡å¤±è´¥: {e}")

async def store_structured_data(task_id: str, file_path: str, analysis_type: str):
    """åå°ä»»åŠ¡ï¼šå­˜å‚¨ç»“æ„åŒ–ä¿¡æ¯ç»™ä¿®å¤agent"""
    global coordinator
    
    try:
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        max_wait_time = 300  # 5åˆ†é’Ÿ
        wait_interval = 2    # 2ç§’
        waited_time = 0
        
        while waited_time < max_wait_time:
            task = coordinator.task_manager.tasks.get(task_id)
            if task and task['status'].value == "completed":
                break
            await asyncio.sleep(wait_interval)
            waited_time += wait_interval
        
        if waited_time >= max_wait_time:
            print(f"ä»»åŠ¡ {task_id} è¶…æ—¶ï¼Œæ— æ³•å­˜å‚¨ç»“æ„åŒ–æ•°æ®")
            return
        
        # è·å–æ£€æµ‹ç»“æœ
        detection_results = (task.get('result') or {}).get("detection_results", {})
        if not detection_results:
            print(f"ä»»åŠ¡ {task_id} æ²¡æœ‰æ£€æµ‹ç»“æœ")
            return
        
        # åˆ›å»ºç»“æ„åŒ–æ•°æ®å­˜å‚¨ç›®å½•
        structured_dir = Path("structured_data")
        structured_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆç»“æ„åŒ–æ•°æ®
        structured_data = {
            "task_id": task_id,
            "file_path": file_path,
            "analysis_type": analysis_type,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_issues": detection_results.get("total_issues", 0),
                "error_count": detection_results.get("summary", {}).get("error_count", 0),
                "warning_count": detection_results.get("summary", {}).get("warning_count", 0),
                "info_count": detection_results.get("summary", {}).get("info_count", 0),
                "languages_detected": detection_results.get("languages_detected", []),
                "total_files": detection_results.get("total_files", 1)
            },
            "issues_by_priority": categorize_issues_by_priority(detection_results.get("issues", [])),
            "fix_recommendations": generate_fix_recommendations(detection_results.get("issues", [])),
            "project_structure": analyze_project_structure(detection_results, analysis_type),
            "detection_metadata": {
                "detection_tools": detection_results.get("detection_tools", []),
                "analysis_time": detection_results.get("analysis_time", 0),
                "project_path": detection_results.get("project_path", file_path)
            }
        }
        
        # ä¿å­˜ç»“æ„åŒ–æ•°æ®
        structured_file = structured_dir / f"structured_data_{task_id}.json"
        with open(structured_file, 'w', encoding='utf-8') as f:
            json.dump(structured_data, f, ensure_ascii=False, indent=2)
        
        print(f"ç»“æ„åŒ–æ•°æ®å·²å­˜å‚¨: {structured_file}")
        
    except Exception as e:
        print(f"å­˜å‚¨ç»“æ„åŒ–æ•°æ®å¤±è´¥: {e}")

async def generate_ai_report(detection_results: Dict[str, Any], file_path: str) -> str:
    """ç”ŸæˆAIåˆ†ææŠ¥å‘Š"""
    try:
        issues = detection_results.get("issues", [])
        total_issues = detection_results.get("total_issues", 0)
        
        if total_issues == 0:
            return "# AIåˆ†ææŠ¥å‘Š\n\n## æ£€æµ‹ç»“æœ\n\nâœ… æœªå‘ç°æ˜æ˜¾çš„ä»£ç ç¼ºé™·ã€‚\n\n## å»ºè®®\n\n- ä»£ç è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒ\n- å¯ä»¥è€ƒè™‘æ·»åŠ æ›´å¤šçš„å•å…ƒæµ‹è¯•\n- å®šæœŸè¿›è¡Œä»£ç å®¡æŸ¥\n"
        
        # æŒ‰ä¸¥é‡æ€§åˆ†ç»„é—®é¢˜
        error_issues = [issue for issue in issues if issue.get("severity") == "error"]
        warning_issues = [issue for issue in issues if issue.get("severity") == "warning"]
        info_issues = [issue for issue in issues if issue.get("severity") == "info"]
        
        report = f"# AIåˆ†ææŠ¥å‘Š\n\n"
        report += f"## æ–‡ä»¶ä¿¡æ¯\n\n- **æ–‡ä»¶è·¯å¾„**: {file_path}\n"
        report += f"- **æ€»é—®é¢˜æ•°**: {total_issues}\n"
        report += f"- **é”™è¯¯**: {len(error_issues)} ä¸ª\n"
        report += f"- **è­¦å‘Š**: {len(warning_issues)} ä¸ª\n"
        report += f"- **ä¿¡æ¯**: {len(info_issues)} ä¸ª\n\n"
        
        # ä¸¥é‡é—®é¢˜åˆ†æ
        if error_issues:
            report += "## ğŸš¨ ä¸¥é‡é—®é¢˜\n\n"
            for issue in error_issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                report += f"### {issue.get('type', 'unknown')}\n"
                report += f"- **ä½ç½®**: ç¬¬{issue.get('line', 0)}è¡Œ\n"
                report += f"- **æè¿°**: {issue.get('message', '')}\n"
                report += f"- **å»ºè®®**: éœ€è¦ç«‹å³ä¿®å¤æ­¤é—®é¢˜\n\n"
        
        # è­¦å‘Šé—®é¢˜åˆ†æ
        if warning_issues:
            report += "## âš ï¸ è­¦å‘Šé—®é¢˜\n\n"
            for issue in warning_issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                report += f"### {issue.get('type', 'unknown')}\n"
                report += f"- **ä½ç½®**: ç¬¬{issue.get('line', 0)}è¡Œ\n"
                report += f"- **æè¿°**: {issue.get('message', '')}\n"
                report += f"- **å»ºè®®**: å»ºè®®ä¿®å¤ä»¥æé«˜ä»£ç è´¨é‡\n\n"
        
        # ä»£ç è´¨é‡å»ºè®®
        report += "## ğŸ’¡ ä»£ç è´¨é‡å»ºè®®\n\n"
        
        # æ ¹æ®é—®é¢˜ç±»å‹ç»™å‡ºå»ºè®®
        issue_types = set(issue.get('type', 'unknown') for issue in issues)
        
        if 'unhandled_exception' in issue_types:
            report += "- **å¼‚å¸¸å¤„ç†**: å»ºè®®æ·»åŠ try-catchå—æ¥å¤„ç†å¯èƒ½çš„å¼‚å¸¸\n"
        
        if 'potential_division_by_zero' in issue_types:
            report += "- **é™¤é›¶æ£€æŸ¥**: å»ºè®®åœ¨é™¤æ³•æ“ä½œå‰æ£€æŸ¥é™¤æ•°æ˜¯å¦ä¸ºé›¶\n"
        
        if 'unused_import' in issue_types:
            report += "- **ä»£ç æ¸…ç†**: å»ºè®®ç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥è¯­å¥\n"
        
        if 'missing_docstring' in issue_types:
            report += "- **æ–‡æ¡£åŒ–**: å»ºè®®ä¸ºå‡½æ•°å’Œç±»æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²\n"
        
        if 'hardcoded_secrets' in issue_types:
            report += "- **å®‰å…¨æ€§**: å»ºè®®å°†ç¡¬ç¼–ç çš„å¯†é’¥ç§»åˆ°ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­\n"
        
        report += "\n## ğŸ“Š æ€»ç»“\n\n"
        
        if len(error_issues) > 0:
            report += f"å‘ç° {len(error_issues)} ä¸ªä¸¥é‡é—®é¢˜éœ€è¦ç«‹å³ä¿®å¤ã€‚\n"
        
        if len(warning_issues) > 0:
            report += f"å‘ç° {len(warning_issues)} ä¸ªè­¦å‘Šé—®é¢˜å»ºè®®ä¿®å¤ã€‚\n"
        
        if len(info_issues) > 0:
            report += f"å‘ç° {len(info_issues)} ä¸ªä¿¡æ¯æç¤ºå¯ä»¥æ”¹è¿›ã€‚\n"
        
        report += "\nå»ºè®®æŒ‰ä¼˜å…ˆçº§é€æ­¥ä¿®å¤è¿™äº›é—®é¢˜ï¼Œä»¥æé«˜ä»£ç è´¨é‡å’Œå¯ç»´æŠ¤æ€§ã€‚\n"
        
        return report
        
    except Exception as e:
        return f"# AIåˆ†ææŠ¥å‘Š\n\n## é”™è¯¯\n\nç”ŸæˆAIæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\n"

def categorize_issues_by_priority(issues):
    """æŒ‰ä¼˜å…ˆçº§åˆ†ç±»é—®é¢˜"""
    priority_categories = {
        "critical": [],  # é”™è¯¯çº§åˆ«ï¼Œå®‰å…¨ç›¸å…³
        "high": [],      # é”™è¯¯çº§åˆ«ï¼Œéå®‰å…¨ç›¸å…³
        "medium": [],    # è­¦å‘Šçº§åˆ«
        "low": []        # ä¿¡æ¯çº§åˆ«
    }
    
    for issue in issues:
        severity = issue.get("severity", "info")
        issue_type = issue.get("type", "")
        
        # å®‰å…¨ç›¸å…³é—®é¢˜ä¼˜å…ˆçº§æœ€é«˜
        if severity == "error" and any(keyword in issue_type.lower() for keyword in 
                                      ["security", "vulnerability", "injection", "xss", "csrf", "secret", "password"]):
            priority_categories["critical"].append(issue)
        elif severity == "error":
            priority_categories["high"].append(issue)
        elif severity == "warning":
            priority_categories["medium"].append(issue)
        else:
            priority_categories["low"].append(issue)
    
    return priority_categories

def generate_fix_recommendations(issues):
    """ç”Ÿæˆä¿®å¤å»ºè®®"""
    recommendations = {
        "immediate_actions": [],
        "short_term_improvements": [],
        "long_term_optimizations": []
    }
    
    error_count = sum(1 for issue in issues if issue.get("severity") == "error")
    warning_count = sum(1 for issue in issues if issue.get("severity") == "warning")
    
    # ç«‹å³è¡ŒåŠ¨
    if error_count > 0:
        recommendations["immediate_actions"].append(f"ä¿®å¤ {error_count} ä¸ªé”™è¯¯çº§åˆ«çš„é—®é¢˜")
    
    # å®‰å…¨ç›¸å…³é—®é¢˜
    security_issues = [issue for issue in issues if "security" in issue.get("type", "").lower()]
    if security_issues:
        recommendations["immediate_actions"].append(f"ä¼˜å…ˆå¤„ç† {len(security_issues)} ä¸ªå®‰å…¨é—®é¢˜")
    
    # çŸ­æœŸæ”¹è¿›
    if warning_count > 10:
        recommendations["short_term_improvements"].append("è¿›è¡Œä»£ç å®¡æŸ¥ï¼Œå¤„ç†å¤§é‡è­¦å‘Š")
    
    # é•¿æœŸä¼˜åŒ–
    recommendations["long_term_optimizations"].append("å»ºç«‹æŒç»­é›†æˆæµç¨‹ï¼Œå®šæœŸè¿›è¡Œä»£ç è´¨é‡æ£€æŸ¥")
    recommendations["long_term_optimizations"].append("åˆ¶å®šä»£ç è§„èŒƒå’Œæœ€ä½³å®è·µæŒ‡å—")
    
    return recommendations

def analyze_project_structure(detection_results, analysis_type):
    """åˆ†æé¡¹ç›®ç»“æ„"""
    structure_info = {
        "analysis_type": analysis_type,
        "file_count": detection_results.get("total_files", 1),
        "languages": detection_results.get("languages_detected", []),
        "complexity_indicators": {
            "high_issue_files": 0,
            "average_issues_per_file": 0
        }
    }
    
    issues = detection_results.get("issues", [])
    if issues:
        # ç»Ÿè®¡æ¯ä¸ªæ–‡ä»¶çš„é—®é¢˜æ•°é‡
        file_issue_count = {}
        for issue in issues:
            file_name = issue.get("file", "unknown")
            file_issue_count[file_name] = file_issue_count.get(file_name, 0) + 1
        
        # è®¡ç®—é«˜é—®é¢˜æ–‡ä»¶æ•°é‡
        structure_info["complexity_indicators"]["high_issue_files"] = sum(
            1 for count in file_issue_count.values() if count > 5
        )
        
        # è®¡ç®—å¹³å‡é—®é¢˜æ•°
        total_files = len(file_issue_count) or 1
        structure_info["complexity_indicators"]["average_issues_per_file"] = len(issues) / total_files
    
    return structure_info

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)