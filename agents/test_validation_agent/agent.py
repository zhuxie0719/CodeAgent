"""
æµ‹è¯•éªŒè¯AGENTä¸»ç±»ï¼ˆé‡æ„ï¼šç»§æ‰¿ BaseAgent å¹¶å®ç°æ ‡å‡†ä»»åŠ¡å¤„ç†æµç¨‹ï¼‰
"""

import asyncio
from typing import Dict, List, Any, Optional
from agents.base_agent import BaseAgent
from .tester import UnitTester, IntegrationTester, PerformanceTester
from .ai_test_generator import AITestGenerator
from .mock_ai_test_generator import MockAITestGenerator


class TestValidationAgent(BaseAgent):
    """æµ‹è¯•éªŒè¯AGENTï¼šè´Ÿè´£å¯¹ä¿®å¤åçš„é¡¹ç›®è¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•ä¸åé¦ˆ"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(agent_id="test_validation_agent", config=config)
        self.unit_tester = UnitTester(config)
        self.integration_tester = IntegrationTester(config)
        self.performance_tester = PerformanceTester(config)
        
        # æ ¹æ®æ˜¯å¦æœ‰APIå¯†é’¥é€‰æ‹©AIç”Ÿæˆå™¨
        api_key = config.get("ai_api_key")
        if api_key:
            self.ai_test_generator = AITestGenerator(api_key=api_key)
            print("ğŸ¤– ä½¿ç”¨çœŸå®AIæµ‹è¯•ç”Ÿæˆå™¨")
        else:
            # å¼ºåˆ¶ä½¿ç”¨Mockç”Ÿæˆå™¨è¿›è¡Œæµ‹è¯•
            self.ai_test_generator = MockAITestGenerator()
            print("ğŸ¤– ä½¿ç”¨æ¨¡æ‹ŸAIæµ‹è¯•ç”Ÿæˆå™¨ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰")
        
        self.task_status = {}  # å­˜å‚¨ä»»åŠ¡çŠ¶æ€

    async def initialize(self) -> bool:
        return True
    
    async def submit_task(self, task_id: str, payload: Dict[str, Any]):
        """æäº¤ä»»åŠ¡ - åè°ƒä¸­å¿ƒè°ƒç”¨çš„æ¥å£"""
        try:
            # è®¾ç½®ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
            self.task_status[task_id] = {
                'status': 'running',
                'result': None,
                'error': None
            }
            
            # å¼‚æ­¥æ‰§è¡Œä»»åŠ¡
            asyncio.create_task(self._execute_task_async(task_id, payload))
            
        except Exception as e:
            self.task_status[task_id] = {
                'status': 'failed',
                'result': None,
                'error': str(e)
            }
    
    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€ - åè°ƒä¸­å¿ƒè°ƒç”¨çš„æ¥å£"""
        return self.task_status.get(task_id)
    
    async def _execute_task_async(self, task_id: str, payload: Dict[str, Any]):
        """å¼‚æ­¥æ‰§è¡Œä»»åŠ¡"""
        try:
            print(f"å¼€å§‹å¤„ç†éªŒè¯ä»»åŠ¡ {task_id}")
            
            # æ‰§è¡ŒéªŒè¯
            validation_result = await self.process_task(task_id, payload)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            self.task_status[task_id] = {
                'status': 'completed',
                'result': validation_result,
                'error': None
            }
            
        except Exception as e:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            self.task_status[task_id] = {
                'status': 'failed',
                'result': None,
                'error': str(e)
            }

    def get_capabilities(self) -> List[str]:
        return [
            "validate_fix",
            "run_unit_tests",
            "run_integration_tests",
            "run_performance_tests",
            "generate_tests_with_ai"
        ]

    async def process_task(self, task_id: str, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç»Ÿä¸€ä»»åŠ¡å…¥å£
        å¤„ç†åè°ƒå™¨å‘é€çš„éªŒè¯ä»»åŠ¡
        task_data ç»“æ„ï¼ˆæ¥è‡ªåè°ƒå™¨ï¼‰ï¼š
        {
            "project_path": str,
            "file_path": str,
            "fix_result": dict,
            "original_issues": list,
            "test_options": dict
        }
        """
        try:
            self.logger.info(f"å¼€å§‹å¤„ç†éªŒè¯ä»»åŠ¡: {task_id}")
            
            # ä»åè°ƒå™¨å‘é€çš„æ•°æ®ä¸­æå–ä¿¡æ¯
            project_path = task_data.get("project_path")
            file_path = task_data.get("file_path") or task_data.get("test_file")  # æ”¯æŒtest_fileå­—æ®µ
            fix_result = task_data.get("fix_result", {})
            test_options = task_data.get("test_options", {})
            
            # å¯¼å…¥osæ¨¡å—
            import os
            
            # å¦‚æœæ²¡æœ‰project_pathï¼Œä½¿ç”¨file_pathçš„ç›®å½•
            if not project_path and file_path:
                project_path = os.path.dirname(file_path)
            
            # å¦‚æœfile_pathä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œåˆ™ç›¸å¯¹äºproject_pathæ„å»ºå®Œæ•´è·¯å¾„
            if file_path and not os.path.isabs(file_path) and project_path:
                file_path = os.path.join(project_path, file_path)
            
            if not project_path:
                raise ValueError("ç¼ºå°‘ project_path æˆ– file_path")

            # è®¾ç½®æµ‹è¯•é€‰é¡¹
            min_coverage = self.config.get("min_coverage", 80)
            generate_with_ai = test_options.get("generate_with_ai", True)  # é»˜è®¤å¯ç”¨AIç”Ÿæˆ

            # å¯é€‰ï¼šåœ¨æµ‹è¯•å‰è¿›è¡ŒAIç”¨ä¾‹ç”Ÿæˆ
            ai_generated_test = None
            if generate_with_ai and file_path:
                print(f"ğŸ¤– ä½¿ç”¨AIç”Ÿæˆæµ‹è¯•æ–‡ä»¶: {file_path}")
                ai_result = await self.ai_test_generator.generate_test_file(file_path, project_path)
                if ai_result["success"]:
                    ai_generated_test = ai_result["test_file_path"]
                    print(f"âœ… AIæµ‹è¯•æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {ai_generated_test}")
                else:
                    print(f"âŒ AIæµ‹è¯•æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {ai_result.get('error', 'æœªçŸ¥é”™è¯¯')}")

            # æ‰§è¡Œå®Œæ•´éªŒè¯
            validation_result = {
                "passed": False,
                "test_results": {},
                "coverage": 0,
                "performance_metrics": {},
                "timestamp": asyncio.get_event_loop().time(),
                "validation_status": "unknown"
            }

            # è¿è¡Œå•å…ƒæµ‹è¯•
            # ä¼˜å…ˆä½¿ç”¨AIç”Ÿæˆçš„æµ‹è¯•æ–‡ä»¶
            target_file = None
            if ai_generated_test:
                import os
                # ä½¿ç”¨å®Œæ•´çš„ç›¸å¯¹è·¯å¾„ï¼Œè€Œä¸æ˜¯ä»…ä»…æ–‡ä»¶å
                target_file = os.path.relpath(ai_generated_test, project_path)
                print(f"ğŸ¯ ä½¿ç”¨AIç”Ÿæˆçš„æµ‹è¯•æ–‡ä»¶: {target_file}")
            elif file_path:
                import os
                target_file = os.path.basename(file_path)
                # å¦‚æœæ˜¯Pythonæ–‡ä»¶ï¼Œè½¬æ¢ä¸ºæµ‹è¯•æ–‡ä»¶å
                if target_file.endswith('.py') and not target_file.startswith('test_'):
                    # å°†æ™®é€šPythonæ–‡ä»¶è½¬æ¢ä¸ºæµ‹è¯•æ–‡ä»¶å
                    target_file = f"test_{target_file}"
            
            unit_results = await self.unit_tester.run_tests(project_path, target_file)
            validation_result["test_results"]["unit"] = unit_results
            
            # æ·»åŠ AIç”Ÿæˆä¿¡æ¯åˆ°ç»“æœä¸­
            if ai_generated_test:
                validation_result["ai_generated_test"] = {
                    "file_path": ai_generated_test,
                    "success": True
                }
                validation_result["ai_generated_test_file"] = ai_generated_test

            # è¿è¡Œé›†æˆæµ‹è¯•
            integration_results = await self.integration_tester.run_tests(project_path)
            validation_result["test_results"]["integration"] = integration_results

            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            performance_results = await self.performance_tester.run_tests(project_path)
            validation_result["performance_metrics"] = performance_results

            # è®¡ç®—ä»£ç è¦†ç›–ç‡
            coverage = await self.unit_tester.calculate_coverage(project_path)
            validation_result["coverage"] = coverage

            # åˆ¤æ–­éªŒè¯æ˜¯å¦é€šè¿‡
            validation_result["passed"] = (
                unit_results.get("passed", False)
                and integration_results.get("passed", False)
                and coverage >= min_coverage
            )
            
            # è®¾ç½®éªŒè¯çŠ¶æ€
            validation_result["validation_status"] = "passed" if validation_result["passed"] else "failed"
            
            # æ·»åŠ å›å½’æ£€æµ‹ï¼ˆç®€åŒ–å®ç°ï¼‰
            validation_result["regression_detected"] = False
            
            # æ¸…ç†AIç”Ÿæˆçš„æµ‹è¯•æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            cleanup_ai_tests = test_options.get("cleanup_ai_tests", False)  # é»˜è®¤ä¸æ¸…ç†
            if cleanup_ai_tests and ai_generated_test:
                await self.ai_test_generator.cleanup_test_file(ai_generated_test)
                print(f"ğŸ§¹ å·²æ¸…ç†AIç”Ÿæˆçš„æµ‹è¯•æ–‡ä»¶: {ai_generated_test}")
            elif ai_generated_test:
                print(f"ğŸ“ AIç”Ÿæˆçš„æµ‹è¯•æ–‡ä»¶å·²ä¿ç•™: {ai_generated_test}")
            
            self.logger.info(f"éªŒè¯ä»»åŠ¡å®Œæˆ: {task_id}, é€šè¿‡: {validation_result['passed']}")
            return validation_result
            
        except Exception as e:
            self.logger.error(f"éªŒè¯ä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {e}")
            return {
                "passed": False,
                "test_results": {},
                "coverage": 0,
                "performance_metrics": {},
                "validation_status": "failed",
                "regression_detected": False,
                "error": str(e),
                "timestamp": asyncio.get_event_loop().time()
            }
