#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„å·¥ä½œæµç¨‹
éªŒè¯æµ‹è¯•éªŒè¯ä»£ç†ç°åœ¨èƒ½æ­£ç¡®æµ‹è¯•ä¿®å¤åçš„æ–‡ä»¶
"""

import asyncio
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from agents.test_validation_agent.agent import TestValidationAgent

async def test_fixed_workflow():
    """æµ‹è¯•ä¿®å¤åçš„å·¥ä½œæµç¨‹"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤åçš„å·¥ä½œæµç¨‹")
    print("=" * 50)
    
    # é…ç½®
    config = {
        "min_coverage": 70,
        "ai_api_key": None  # ä½¿ç”¨Mockç”Ÿæˆå™¨
    }
    
    # åˆ›å»ºæµ‹è¯•éªŒè¯ä»£ç†
    test_agent = TestValidationAgent(config=config)
    await test_agent.start()
    
    # æµ‹è¯•æ•°æ® - ä½¿ç”¨ä¿®å¤åçš„æ–‡ä»¶
    test_data = {
        "project_path": str(PROJECT_ROOT),
        "file_path": str(PROJECT_ROOT / "tests" / "output" / "test_python_bad_after.py"),
        "fix_result": {
            "success": True,
            "fix_results": [{
                "file": "test_python_bad.py",
                "before": "test_python_bad_before.py", 
                "after": "test_python_bad_after.py",
                "issues_fixed": 10
            }]
        },
        "test_options": {
            "generate_with_ai": True,
            "min_coverage": 70
        }
    }
    
    print(f"ğŸ“„ æµ‹è¯•æ–‡ä»¶: {test_data['file_path']}")
    print(f"ğŸ“ é¡¹ç›®è·¯å¾„: {test_data['project_path']}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(test_data['file_path']):
        print(f"âŒ ä¿®å¤åçš„æ–‡ä»¶ä¸å­˜åœ¨: {test_data['file_path']}")
        return False
    
    print("âœ… ä¿®å¤åçš„æ–‡ä»¶å­˜åœ¨")
    
    # æ‰§è¡Œæµ‹è¯•éªŒè¯
    try:
        print("\nğŸš€ å¼€å§‹æ‰§è¡Œæµ‹è¯•éªŒè¯...")
        result = await test_agent.process_task("test_task", test_data)
        
        print("\nğŸ“Š æµ‹è¯•éªŒè¯ç»“æœ:")
        print(f"   é€šè¿‡: {result.get('passed', False)}")
        print(f"   çŠ¶æ€: {result.get('validation_status', 'unknown')}")
        print(f"   è¦†ç›–ç‡: {result.get('coverage', 0)}%")
        
        # æ˜¾ç¤ºå•å…ƒæµ‹è¯•ç»“æœ
        unit_results = result.get('test_results', {}).get('unit', {})
        print(f"   å•å…ƒæµ‹è¯•: {'é€šè¿‡' if unit_results.get('passed', False) else 'å¤±è´¥'}")
        if unit_results.get('stdout'):
            print(f"   è¾“å‡º: {unit_results['stdout'][:200]}...")
        if unit_results.get('stderr'):
            print(f"   é”™è¯¯: {unit_results['stderr'][:200]}...")
        
        # æ˜¾ç¤ºAIç”Ÿæˆçš„æµ‹è¯•æ–‡ä»¶
        if 'ai_generated_test_file' in result:
            print(f"   AIæµ‹è¯•æ–‡ä»¶: {result['ai_generated_test_file']}")
        
        return result.get('passed', False)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        await test_agent.stop()

async def test_original_vs_fixed():
    """å¯¹æ¯”æµ‹è¯•åŸå§‹æ–‡ä»¶å’Œä¿®å¤åçš„æ–‡ä»¶"""
    print("\nğŸ”„ å¯¹æ¯”æµ‹è¯•: åŸå§‹æ–‡ä»¶ vs ä¿®å¤åçš„æ–‡ä»¶")
    print("=" * 50)
    
    config = {"min_coverage": 70, "ai_api_key": None}
    test_agent = TestValidationAgent(config=config)
    await test_agent.start()
    
    # æµ‹è¯•åŸå§‹æ–‡ä»¶
    original_data = {
        "project_path": str(PROJECT_ROOT),
        "file_path": str(PROJECT_ROOT / "tests" / "test_python_bad.py"),
        "test_options": {"generate_with_ai": True, "min_coverage": 70}
    }
    
    # æµ‹è¯•ä¿®å¤åçš„æ–‡ä»¶
    fixed_data = {
        "project_path": str(PROJECT_ROOT),
        "file_path": str(PROJECT_ROOT / "tests" / "output" / "test_python_bad_after.py"),
        "test_options": {"generate_with_ai": True, "min_coverage": 70}
    }
    
    try:
        print("ğŸ“„ æµ‹è¯•åŸå§‹æ–‡ä»¶...")
        original_result = await test_agent.process_task("original_test", original_data)
        print(f"   ç»“æœ: {'é€šè¿‡' if original_result.get('passed', False) else 'å¤±è´¥'}")
        
        print("\nğŸ“„ æµ‹è¯•ä¿®å¤åçš„æ–‡ä»¶...")
        fixed_result = await test_agent.process_task("fixed_test", fixed_data)
        print(f"   ç»“æœ: {'é€šè¿‡' if fixed_result.get('passed', False) else 'å¤±è´¥'}")
        
        print(f"\nğŸ“Š å¯¹æ¯”ç»“æœ:")
        print(f"   åŸå§‹æ–‡ä»¶: {'âœ… é€šè¿‡' if original_result.get('passed', False) else 'âŒ å¤±è´¥'}")
        print(f"   ä¿®å¤æ–‡ä»¶: {'âœ… é€šè¿‡' if fixed_result.get('passed', False) else 'âŒ å¤±è´¥'}")
        
        return fixed_result.get('passed', False)
        
    except Exception as e:
        print(f"âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    finally:
        await test_agent.stop()

if __name__ == "__main__":
    async def main():
        print("ğŸ¯ æµ‹è¯•ä¿®å¤åçš„å·¥ä½œæµç¨‹")
        
        # æµ‹è¯•1: ä¿®å¤åçš„å·¥ä½œæµç¨‹
        success1 = await test_fixed_workflow()
        
        # æµ‹è¯•2: å¯¹æ¯”æµ‹è¯•
        success2 = await test_original_vs_fixed()
        
        print(f"\nğŸ æµ‹è¯•æ€»ç»“:")
        print(f"   ä¿®å¤åå·¥ä½œæµç¨‹: {'âœ… æˆåŠŸ' if success1 else 'âŒ å¤±è´¥'}")
        print(f"   å¯¹æ¯”æµ‹è¯•: {'âœ… æˆåŠŸ' if success2 else 'âŒ å¤±è´¥'}")
        
        if success1 and success2:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¿®å¤åçš„å·¥ä½œæµç¨‹æ­£å¸¸å·¥ä½œã€‚")
        else:
            print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    
    asyncio.run(main())
